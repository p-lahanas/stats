[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Kernel Density Estimation (KDE)\n\n\n\nwrite-up\n\ntheory\n\n\n\n\n\n\n\n\n\nAug 11, 2025\n\n\nPeter Lahanas\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem (CLT)\n\n\n\nwrite-up\n\ntheory\n\n\n\n\n\n\n\n\n\nAug 9, 2025\n\n\nPeter Lahanas\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Statistics Blog\n\n\n\nwelcome\n\n\n\n\n\n\n\n\n\nAug 6, 2025\n\n\nPeter Lahanas\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/clt/clt.html",
    "href": "posts/clt/clt.html",
    "title": "Central Limit Theorem (CLT)",
    "section": "",
    "text": "Why do so many things seem to follow some sort of a normal distribution? Well, one possible explanation is the central limit theorem.\n\nFormally:\nLet’s define a set of independent random variables \\(\\{X_{1},X_{2},..., X_{n}\\}\\) which are identically distributed, have a mean \\(\\mu=0\\) and variance \\(\\sigma^2=1\\). If we define \\(\\bar X_n = \\frac{1}{n} \\sum_{i=0}^{n} X_i\\), we can say that the normalised mean \\(\\sqrt{n}(\\bar X_n - \\mu)\\) approaches the normal distribution while n approaches infinity.\n\\[\\sqrt{n}(\\bar X_n - \\mu) \\xrightarrow{d} \\mathcal{N}(0,\\sigma^2)\\]\n\n\nMore simply:\nThe sampling distribution of a sample mean is approximately normal if the sample size is large enough. So, if we calculate many sample means over a distribution (regardless of the distribution), the distribution of the sample means will be somewhat normally distributed.\nLet’s give it a try:\n\nimport numpy as np\nimport seaborn as sns\n\n# Generate a sample mean from a uniform distribution\ndef generate_sample_mean_uniform(num_samples:int =100) -&gt; np.ndarray:\n    return np.random.randint(1,20, size=num_samples).mean()\n\n# Generate multiple sample means from our uniform distribution\ndef generate_sample_means_uniform(samples: int = 500) -&gt; np.ndarray:\n    return np.array([generate_sample_mean_uniform() for _ in range(samples)])\n\n\nsns.histplot(generate_sample_means_uniform())\n\n\n\n\n\n\n\nFigure 1: The distribution of sample means from an underlying uniform distribution"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a place for me to learn more about statistics and share my findings."
  },
  {
    "objectID": "posts/kde/kde.html",
    "href": "posts/kde/kde.html",
    "title": "Kernel Density Estimation (KDE)",
    "section": "",
    "text": "KDEs help us answer the question: how do we estimate the probability density function (PDF) based on observed data?\nThis method is non-parametric, meaning we do not make an assumption about the underlying distribution. Let’s start with histograms and further refine our model."
  },
  {
    "objectID": "posts/kde/kde.html#histograms",
    "href": "posts/kde/kde.html#histograms",
    "title": "Kernel Density Estimation (KDE)",
    "section": "Histograms",
    "text": "Histograms\nHistograms are a good starting point for density estimation as they are relatively easy to make. To construct a histogram, we divide the observed data interval into adjacent, consecutive groups called ‘bins’. These are placed on the x-axis and the number of observations that fall into each bin give us the y values."
  },
  {
    "objectID": "posts/kde/kde.html#kde",
    "href": "posts/kde/kde.html#kde",
    "title": "Kernel Density Estimation (KDE)",
    "section": "KDE",
    "text": "KDE\n\nIs a composite function made up of one kind of building block referred to as a kernel function.\nThe kernel function is evaluated for each datapoint separately, and these partial results are summed to form the KDE\n\n\nFirst Principles\nLet’s start with the simplest example, take one data point x = 0. A logical PDF for a single data point would be a peak that is precisely over the data point and decays the further away we get. Take \\(exp(-x^2)\\) but… we need to rescale so that the area under our PDF is 1. So we end up with a kernel function that looks something like this. I’m not sure exactly how this was derived (it’s a Gaussian distribution with mean 0 and std. deviation of 1) I stole it from here who I believe stole it from here. \\[K(x) = \\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{x^2}{2})\\]\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nx = np.linspace(-3, 3, 100)\n\ny_1 = np.exp(-1 *(np.square(x))) # exp(-x^2)\ny_2 = (1/(np.sqrt(2*np.pi)) * np.exp(-1*(np.square(x)/2))) # exp((1/sqrt(2pi)) * exp(-(x^2/2))\n\n\nplt.plot(x, np.c_[y_1, y_2], label=[r'$exp(-x^2)$',r'$\\frac{1}{\\sqrt{2\\pi}} exp(-\\frac{x^2}{2})$']);\nplt.legend()\n\n# Add our kernel functions\nplt.plot()\n\n\n\n\n\n\n\n\nLet’s add some extra parameters. Consider a dataset \\(X\\). Let \\(x_i\\) be the \\(i\\) th observation in that dataset. \\(x_i\\) shifts our kernel across the x axis and the kernel bandwidth, \\(h\\) changes the shape of our curve.\nNote: we divide our kernel function by \\(h\\) to restore our PDF back to unit area. Now, we can estimate the PDF of any point \\(x_i\\) using. \\[\\frac{1}{h}K(\\frac{x-x_i}{h})\\]\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, axs = plt.subplots(3, 1, figsize=(10,8))\n\nx_points = [-0.75, 0.1, 1.5]\ny_points = [0, 0, 0]\nh = [0.2, 1, 0.5]\npoint_labels = [\"x\" + str(i+1) for i in range(len(x_points))]\ncolours = [\"m\", \"g\", \"y\"]\n\nfor i in range(3):\n    # Plot our data points\n    axs[i].scatter(x_points[i], y_points[i], color=\"black\", marker=\"s\")\n    axs[i].text(x_points[i]+0.09, y_points[i], point_labels[i])\n\n    # Plot our PDF\n    x = np.linspace(-1, 2, 300)\n    y = (1/h[i]) * (1/(np.sqrt(2*np.pi))) * np.exp(-1*(np.square(((x-x_points[i])/h[i]*2))))\n    axs[i].plot(x, y, linestyle=\"-\", label=f\"h={h[i]}, x_i={x_points[i]}\", color=colours[i])\n    axs[i].legend()\n\n\n\n\n\n\n\n\nHow do we estimate the PDF across our entire dataset, \\(X\\)? Let’s extend our previous function to generalise to \\(n\\) data points\nTODO: the rest is a work in progress.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1)\n\nx_points = [0, 1.5, -1]\ny_points = [0, 0, 0]\npoint_labels = [\"x\" + str(i+1) for i in range(len(x_points))]\n\n\n# Add points and labels\nplt.scatter(x_points, y_points, color=\"black\", marker=\"s\")\nfor i in range(len(x_points)):\n    plt.text(x_points[i]+0.09, y_points[i], point_labels[i])\n\n# Add our kernel functions\nplt.plot()\n\n\\(K(x) = 1/sqrt(2pi) exp[-(x^2)/2]\\)\nThis is our Kernel function and is a valid PDF. Effectively a Gaussian distribution with mean 0 and unit variance\nThis is our Gaussian kernel. If we expand it to N datapoints and scale back our PDF, we get a smoother fit of our PDF.\nTODO: - Show a script deriving from first principles using a Guassian Kernel function (like in the towardsdatascience paper) - Use seaborn as a more polished/modern/faster way of doing it. - Look into scikit learn and the KernelDensity function that is has. (you can choose different kernels)"
  },
  {
    "objectID": "posts/kde/kde.html#useful-reading",
    "href": "posts/kde/kde.html#useful-reading",
    "title": "Kernel Density Estimation (KDE)",
    "section": "Useful Reading",
    "text": "Useful Reading\nhttps://www.mvstat.net/tduong/research/seminrs/seminar-2001-05/ https://towardsdatascience.com/kerneldensit-estimation-explained-step-by-step-7cc5b5bc4517/ https://chriskhanhtran.github.io/_poss/202001-13-portfolio-tutorial/ https://quarto.org/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Statistics Blog",
    "section": "",
    "text": "I’ve created this blog as a reference point for some interesting concepts in statistics. Hopefully you find something here useful!"
  }
]